{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SST Classification Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from nltk.corpus import stopwords\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_fine_grained</th>\n",
       "      <th>label_coarse_grained</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>apparently reassembled from the cutting-room f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>the entire movie is filled with deja vu moments .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5855</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>but the talented cast alone will keep you watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5856</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>earnest , unsubtle and hollywood-predictable ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5857</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>jeffrey tambor 's performance as the intellige...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5858</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>an overly familiar scenario is made fresh by a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5859</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>a clash between the artificial structure of th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5860 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label_fine_grained  label_coarse_grained  \\\n",
       "0                      4                     1   \n",
       "1                      1                    -1   \n",
       "2                      1                    -1   \n",
       "3                      2                     0   \n",
       "4                      3                     1   \n",
       "...                  ...                   ...   \n",
       "5855                   4                     1   \n",
       "5856                   3                     1   \n",
       "5857                   4                     1   \n",
       "5858                   3                     1   \n",
       "5859                   1                    -1   \n",
       "\n",
       "                                               sentence  \n",
       "0     a stirring , funny and finally transporting re...  \n",
       "1     apparently reassembled from the cutting-room f...  \n",
       "2     they presume their audience wo n't sit still f...  \n",
       "3     the entire movie is filled with deja vu moments .  \n",
       "4     this is a visually stunning rumination on love...  \n",
       "...                                                 ...  \n",
       "5855  but the talented cast alone will keep you watc...  \n",
       "5856  earnest , unsubtle and hollywood-predictable ,...  \n",
       "5857  jeffrey tambor 's performance as the intellige...  \n",
       "5858  an overly familiar scenario is made fresh by a...  \n",
       "5859  a clash between the artificial structure of th...  \n",
       "\n",
       "[5860 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('stsa.fine.train.converted.csv', sep=',')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5860"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the number of messages in the dataset \n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels_grained = train_data['label_fine_grained']\n",
    "trainLabels_coarse = train_data['label_coarse_grained']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of values for fine grained labels\n",
    "len(train_data['label_fine_grained'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1607\n",
       "1    1515\n",
       "2    1117\n",
       "4     860\n",
       "0     761\n",
       "Name: label_fine_grained, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the distribution of messages for fine grained labels\n",
    "trainLabels_grained.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of values for coarse grained labels\n",
    "len(train_data['label_coarse_grained'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    2467\n",
       "-1    2276\n",
       " 0    1117\n",
       "Name: label_coarse_grained, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the distribution of messages for coarse grained labels\n",
    "trainLabels_coarse.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vectorizer_unigram = CountVectorizer(stop_words='english')\n",
    "cnt_unigram_train_vector = cnt_vectorizer_unigram.fit_transform(train_data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.626279863481228"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the average number of tokens per message\n",
    "import numpy as np\n",
    "tokens = cnt_unigram_train_vector.toarray()\n",
    "n_tokens = 0\n",
    "for i in range(len(tokens)): \n",
    "    \n",
    "    count_arr = np.count_nonzero(tokens[i] == 1)\n",
    "    n_tokens += count_arr \n",
    "avg_tokens = n_tokens / len(tokens)\n",
    "avg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_fine_grained</th>\n",
       "      <th>label_coarse_grained</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>no movement , no yuks , not much of anything .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>` how many more voyages can this limping but d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>so relentlessly wholesome it made me want to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>the problem with concept films is that if the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>safe conduct , however ambitious and well-inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>a film made with as little wit , interest , an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>to enjoy this movie 's sharp dialogue and deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>but here 's the real damn : it is n't funny , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2210 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label_fine_grained  label_coarse_grained  \\\n",
       "0                      1                    -1   \n",
       "1                      0                    -1   \n",
       "2                      2                     0   \n",
       "3                      2                     0   \n",
       "4                      0                    -1   \n",
       "...                  ...                   ...   \n",
       "2205                   1                    -1   \n",
       "2206                   1                    -1   \n",
       "2207                   1                    -1   \n",
       "2208                   2                     0   \n",
       "2209                   0                    -1   \n",
       "\n",
       "                                               sentence  \n",
       "0        no movement , no yuks , not much of anything .  \n",
       "1     a gob of drivel so sickly sweet , even the eag...  \n",
       "2     ` how many more voyages can this limping but d...  \n",
       "3     so relentlessly wholesome it made me want to s...  \n",
       "4     gangs of new york is an unapologetic mess , wh...  \n",
       "...                                                 ...  \n",
       "2205  the problem with concept films is that if the ...  \n",
       "2206  safe conduct , however ambitious and well-inte...  \n",
       "2207  a film made with as little wit , interest , an...  \n",
       "2208  to enjoy this movie 's sharp dialogue and deli...  \n",
       "2209  but here 's the real damn : it is n't funny , ...  \n",
       "\n",
       "[2210 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the test dataset\n",
    "test_data = pd.read_csv('stsa.fine.test.converted.csv', sep=',')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_fine_grained</th>\n",
       "      <th>label_coarse_grained</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>in his first stab at the form , jacquot takes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>one long string of cliches .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>if you 've ever entertained the notion of doin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>k-19 exploits our substantial collective fear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>it 's played in the most straight-faced fashio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>`` the ring '' is pretty much an english-langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>smart , provocative and blisteringly funny .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>this one is definitely one to skip , even for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>charles ' entertaining film chronicles seinfel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>an effectively creepy , fear-inducing -lrb- no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1101 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label_fine_grained  label_coarse_grained  \\\n",
       "0                      2                     0   \n",
       "1                      1                    -1   \n",
       "2                      1                    -1   \n",
       "3                      0                    -1   \n",
       "4                      1                    -1   \n",
       "...                  ...                   ...   \n",
       "1096                   2                     0   \n",
       "1097                   3                     1   \n",
       "1098                   0                    -1   \n",
       "1099                   3                     1   \n",
       "1100                   4                     1   \n",
       "\n",
       "                                               sentence  \n",
       "0     in his first stab at the form , jacquot takes ...  \n",
       "1                          one long string of cliches .  \n",
       "2     if you 've ever entertained the notion of doin...  \n",
       "3     k-19 exploits our substantial collective fear ...  \n",
       "4     it 's played in the most straight-faced fashio...  \n",
       "...                                                 ...  \n",
       "1096  `` the ring '' is pretty much an english-langu...  \n",
       "1097       smart , provocative and blisteringly funny .  \n",
       "1098  this one is definitely one to skip , even for ...  \n",
       "1099  charles ' entertaining film chronicles seinfel...  \n",
       "1100  an effectively creepy , fear-inducing -lrb- no...  \n",
       "\n",
       "[1101 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the dev dataset\n",
    "dev_data = pd.read_csv('stsa.fine.dev.converted.csv', sep=',')\n",
    "dev_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(random_state=42, kernel='linear')\n",
    "trained_model = clf.fit(cnt_unigram_train_vector, trainLabels_grained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "devLabels_coarse = dev_data['label_coarse_grained']\n",
    "testLabels_coarse = test_data['label_coarse_grained']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementing FeatureUnion()to a Pipeline() using CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer_union = FeatureUnion([('cnt_word', CountVectorizer(stop_words='english')),\n",
    "                               ('cnt_char', CountVectorizer(analyzer='char', ngram_range=(1, 2)))\n",
    "                               ])\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "            ('vectorize', vectorizer_union),\n",
    "            ('classify', SVC(random_state=42, kernel='linear'))\n",
    "            ])\n",
    "\n",
    "#training the model \n",
    "trained_model_feature_union_cnt = svm_pipeline.fit(train_data['sentence'], trainLabels_coarse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1 -1 ...  1  0 -1] [-1 -1  1 ...  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "prediction_dev_cnt = trained_model_feature_union_cnt.predict(dev_data['sentence'])\n",
    "prediction_test_cnt = trained_model_feature_union_cnt.predict(test_data['sentence'])\n",
    "print(prediction_dev_cnt,prediction_test_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced model - WORD UNIGRAM + CHAR BI-GRAM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.62      0.59       428\n",
      "           0       0.26      0.21      0.23       229\n",
      "           1       0.63      0.64      0.63       444\n",
      "\n",
      "    accuracy                           0.54      1101\n",
      "   macro avg       0.49      0.49      0.49      1101\n",
      "weighted avg       0.53      0.54      0.53      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Enhanced model - WORD UNIGRAM + CHAR BI-GRAM\")\n",
    "print(classification_report(devLabels_coarse, prediction_dev_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementing FeatureUnion()to a Pipeline() using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_unigram = TfidfVectorizer(stop_words='english')\n",
    "tfidf_unigram_train_vector = tfidf_vectorizer_unigram.fit_transform(train_data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_union = FeatureUnion([('tfidf_word', TfidfVectorizer(stop_words='english')),\n",
    "                               ('tfidf_char', TfidfVectorizer(analyzer='char', ngram_range=(1, 2)))\n",
    "                               ])\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "            ('vectorize', vectorizer_union),\n",
    "            ('classify', SVC(random_state=42, kernel='linear'))\n",
    "            ])\n",
    "\n",
    "trained_model_feature_union_tfidf = svm_pipeline.fit(train_data['sentence'], trainLabels_coarse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1 -1 ...  1  0 -1] [-1 -1  1 ... -1  1  1]\n"
     ]
    }
   ],
   "source": [
    "prediction_dev_tfidf = trained_model_feature_union_tfidf.predict(dev_data['sentence'])\n",
    "prediction_test_tfidf = trained_model_feature_union_tfidf.predict(test_data['sentence'])\n",
    "print(prediction_dev_tfidf,prediction_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced model - WORD UNIGRAM + CHAR BI-GRAM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.69      0.65       428\n",
      "           0       0.31      0.13      0.18       229\n",
      "           1       0.63      0.74      0.68       444\n",
      "\n",
      "    accuracy                           0.59      1101\n",
      "   macro avg       0.52      0.52      0.50      1101\n",
      "weighted avg       0.55      0.59      0.56      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Enhanced model - WORD UNIGRAM + CHAR BI-GRAM\")\n",
    "print(classification_report(devLabels_coarse, prediction_dev_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementing FeatureUnion()to a Pipeline() using CountVictorizer and TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_union_mixed = FeatureUnion([('tfidf_word', TfidfVectorizer(stop_words='english')),\n",
    "                               ('cnt_char', CountVectorizer(analyzer='char', ngram_range=(1, 2)))\n",
    "                               ])\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "            ('vectorize', vectorizer_union),\n",
    "            ('classify', SVC(random_state=42, kernel='linear'))\n",
    "            ])\n",
    "\n",
    "trained_model_feature_union_mixed = svm_pipeline.fit(train_data['sentence'], trainLabels_coarse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1 -1 ...  1  0 -1] [-1 -1  1 ... -1  1  1]\n"
     ]
    }
   ],
   "source": [
    "prediction_dev_mixed = trained_model_feature_union_mixed.predict(dev_data['sentence'])\n",
    "prediction_test_mixed = trained_model_feature_union_mixed.predict(test_data['sentence'])\n",
    "print(prediction_dev_mixed,prediction_test_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced model - WORD UNIGRAM + CHAR BI-GRAM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.69      0.65       428\n",
      "           0       0.31      0.13      0.18       229\n",
      "           1       0.63      0.74      0.68       444\n",
      "\n",
      "    accuracy                           0.59      1101\n",
      "   macro avg       0.52      0.52      0.50      1101\n",
      "weighted avg       0.55      0.59      0.56      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Enhanced model - WORD UNIGRAM + CHAR BI-GRAM\")\n",
    "print(classification_report(devLabels_coarse, prediction_dev_mixed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF model has the highest accuracy and therefore we will use it for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced model - WORD UNIGRAM + CHAR BI-GRAM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.65      0.72      0.69       912\n",
      "           0       0.26      0.10      0.15       389\n",
      "           1       0.67      0.78      0.72       909\n",
      "\n",
      "    accuracy                           0.64      2210\n",
      "   macro avg       0.53      0.53      0.52      2210\n",
      "weighted avg       0.59      0.64      0.61      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Enhanced model - WORD UNIGRAM + CHAR BI-GRAM\")\n",
    "print(classification_report(testLabels_coarse, prediction_test_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the best model to the phrases_dataset.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For Nik, he only wants to silence the cacophon...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I can play this two ways</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mild, because it isn't conclusive, and doesn't...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You can also get some more information about t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soon, Hero, who has never had friends, is thru...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  score\n",
       "0  For Nik, he only wants to silence the cacophon...    0.0\n",
       "1                          \"I can play this two ways    0.0\n",
       "2  Mild, because it isn't conclusive, and doesn't...   -1.0\n",
       "3  You can also get some more information about t...    0.0\n",
       "4  Soon, Hero, who has never had friends, is thru...    0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exploring the dataset \n",
    "import csv\n",
    "dataset_dh = pd.read_csv('phrases_dataset.tsv', delimiter=\"\\t\", quoting=csv.QUOTE_NONE, header=None).dropna()\n",
    "dataset_dh.columns =['sentence', 'score']\n",
    "dataset_dh.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  0 -1 -1 -1 -1 -1 -1  0 -1  1 -1 -1  0  1  1 -1  1  1 -1 -1 -1  1\n",
      " -1 -1  1 -1  0 -1  1  1 -1 -1  1  1  1  1  1 -1 -1  1  1  1 -1  1  1  1\n",
      "  1 -1 -1 -1 -1 -1  1  1  1  1  0 -1 -1 -1  1 -1 -1  1 -1 -1 -1  1  1  0\n",
      "  1 -1 -1  1 -1 -1  1 -1  1 -1  1  1 -1 -1 -1 -1  0  1  0  1  1  1 -1  1\n",
      " -1 -1 -1 -1 -1  1 -1 -1  1  1 -1 -1 -1  0  1 -1  1  0  1  1 -1 -1 -1  1\n",
      "  1  1  1  1  0  1 -1 -1  1 -1 -1 -1  1 -1 -1  1  1 -1  1  1 -1  1 -1 -1\n",
      " -1 -1  1  1  1  1  1 -1 -1 -1  1  1  1 -1 -1  0 -1  1 -1  1 -1 -1  1 -1\n",
      "  1 -1  1 -1  1  1  1 -1 -1  1  1 -1 -1 -1  1  1 -1 -1 -1 -1  1  1 -1  1\n",
      " -1 -1  1 -1 -1 -1 -1 -1 -1  1 -1 -1  1  1 -1  1 -1 -1  1  0 -1 -1  1 -1\n",
      "  1 -1  1  1 -1 -1  1  1  0 -1  1  0 -1  1 -1 -1  0  1  1 -1  1 -1  1 -1\n",
      " -1 -1  1 -1 -1  1  1 -1 -1  1 -1 -1 -1 -1  1 -1 -1  1 -1 -1  1 -1 -1 -1\n",
      " -1  0  1 -1 -1 -1  1 -1  1 -1  0  1  1  0 -1  0  1  0  1  1  1 -1  0 -1\n",
      " -1 -1  1 -1  1  1 -1  1  1 -1  0  1  1 -1 -1 -1 -1 -1  1 -1  1  1  1 -1\n",
      "  1  1  1  1 -1 -1  1 -1  0 -1 -1  1 -1 -1  1 -1  1  0  1  1  1  1  1  1\n",
      " -1  0 -1  0 -1  1  0  1 -1 -1 -1 -1  1  1 -1  1 -1  1 -1 -1 -1 -1 -1  1\n",
      " -1  1  1  1  1  1  1 -1 -1 -1  1 -1  1 -1  1 -1  0 -1 -1  1  1 -1 -1 -1\n",
      " -1  1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "#applying the model \n",
    "prediction_test_dh_tfidf = trained_model_feature_union_tfidf.predict(dataset_dh['sentence'])\n",
    "print(prediction_test_dh_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced model - WORD UNIGRAM + CHAR BI-GRAM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.23      0.74      0.36        62\n",
      "         0.0       0.50      0.10      0.17       146\n",
      "         1.0       0.61      0.55      0.58       180\n",
      "\n",
      "    accuracy                           0.41       388\n",
      "   macro avg       0.45      0.46      0.37       388\n",
      "weighted avg       0.51      0.41      0.39       388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Enhanced model - WORD UNIGRAM + CHAR BI-GRAM\")\n",
    "print(classification_report(dataset_dh['score'], prediction_test_dh_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(trained_model_feature_union_tfidf, open(\"feature_unione.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
